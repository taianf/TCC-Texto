
\com
Resumidamente, \doris{} é um protocolo destinado a comunicação
em sistemas de tempo real híbridos que tanto oferece garantias temporais críticas
para as aplicações de tempo real críticas, quanto permitindo o uso eficiente da
banda pelas aplicações não críticas. Para segmento isolados, envolvendo um número
limitado de dispositivos, mostrou-se que o modelo de comunicação um-para-todos é
particularmente bem adaptado para implementar mecanismos de sincronizações temporais
simples e métodos de tolerância a falhas eficiente. Através da especificação formal
do protocolo e de sua verificação automática, as propriedades temporais mais
relevantes foram verificadas.
\moc

\subsection{Estrutura do capitulo}

Após esta introdução, a  seção \ref{sec:SOeTR} deste capítulo abordara os
desafios do desenvolvimento de um Sistema de Tempo Real usando arquiteturas de
computadores comuns. Na seção \ref{sec:escal}, apresentar-se-á brevemente alguns
elementos da teoria de escalonamento em sistemas de tempo real, e em seguida, as
limitações dos Sistemas Operacionais modernos serão discutidas na seção
\ref{sec:TReSOPG}.

Na seção \ref{sec:LinuxSOPG}, encontrar-se-á uma breve descrição das
principais abstrações envolvidas no funcionamento de um Sistema Operacional de
Propósito Geral (SOPG) tal como Linux. Antes disto, as motivações da escolha da
plataforma Linux serão descritas na seção \ref{sec:porque}.

Depois da seção \ref{sec:latencias} na qual as principais causas de latências dos
SOPG serão detalhadas, três abordagens diferentes para aumentar a previsibilidade do
sistema operacional Linux serão apresentadas na parte \ref{sec:sotrLinux}.

A seção \ref{sec:conclusaoPlat} concluirá brevemente este capítulo.







\section{Sistemas Operacionais e Tempo Real} %2
\label{sec:SOeTR}

Dentro do conjunto dos sistemas computacionais, os sistemas chamados de
\textbf{tempo real} são aqueles sistemas especificamente concebidos para atender as
necessidades das aplicações com requisitos temporais. Além de garantir a correção
dos resultados gerados, um Sistema de Tempo Real (STR) deve também garantir os
tempos de execução das tarefas e os prazos de entrega dos resultados.  Portanto, num
sistema de tempo real, as durações e os instantes das ações devem ser conhecidos de
tal forma que o comportamento do sistema seja determinístico e previsível. Para este
efeito, o problema do escalonamento correto dos programas deve ser
resolvido. Portanto, ir-se-á introduzir brevemente elementos da teoria de
escalonamento na próxima seção \ref{sec:escal}.
% antes a seguir para depois caracterizar as limitações atuais do Sistemas
% Operacionais de Propósito Geral (SOPG) na seção \ref{sec:SOPG}.

\subsection{Escalonamento em Sistemas de Tempo Real}
\label{sec:escal}

Para o objetivo da sua análise temporal, um Sistema de Tempo Real é geralmente
modelado por um conjunto de tarefas cuja execução tem restrições temporais. Vários
atributos são utilizados tanto para organizar o escalonamento das tarefas no
processador quanto para analisar a capacidade de um sistema em escalonar um conjunto
de tarefas.  Exemplos de tais atributos para uma tarefa $i$ são: o seu custo
computacional máximo ($C_i$), o seu prazo máximo de execução (\ing{deadlines})
($D_i$), o seu período ($T_i$).  Usando estes atributos, a prioridade ($P_i$) de
cada tarefa é definida de tal forma que o papel do escalonador é simplesmente de
escolher, entre as tarefas prontas para executar, aquela que possui a maior
prioridade.

Para determinar a prioridade de uma tarefa, distingui-se duas classes de políticas
baseadas em prioridades fixas ou variáveis.  Escalonamento baseado em prioridades
fixas assumem que a prioridade de cada tarefa é definida em tempo de
projeto. Exemplos deste tipo de política é o \emph{Rate Monotonic} \cite{Liu73} (RM)
que atribui prioridades em ordem inversa do períodos das tarefas.  Nas políticas
baseadas em prioridades variáveis, a ordem relativa das tarefas prontas para
executar só é conhecida em tempo de execução. A política \ing{Earliest Deadline
  First} (EDF) \cite{Liu73}, por exemplo, escolhe, entre as tarefas prontas para
executar, aquela cujo prazo de execução esteja mais próximo a vencer.

Dado um conjunto de tarefas e uma política de escalonamento, a teoria da análise de
escalonamento se preocupa em determinar as condições necessárias e suficientes para
que o sistema esteja temporalmente correto, ou seja, que não haja violação de
\ing{deadlines} por parte de nenhuma tarefa. Distingui-se as abordagens que usam o
cálculo da utilização máxima do processador \cite{Liu73} e aquelas que verificam se
os tempos de resposta das tarefas, no pior caso da suas execuções, ultrapassam seus
\ing{deadlines} \cite{Audsley93, Tindell94}.
 
Considerando um sistema mono-processador no qual $n$ tarefas são escalonadas com
possibilidade de preempção, a taxa de utilização de cada tarefa vale no pior caso
$C_i/T_i$.  Portanto, o fator de utilização máximo desse conjunto de tarefas é dado
por
\begin{equation}
  \label{eq:utTeste}
  U = \sum_{i = 1}^n \frac{C_i}{T_i}
modifi\end{equation}

Num modelo simplificado do sistema (independência das tarefas, $D_i=T_i$,
etc. \cite{Liu73}), a condição $U \leq 1$ é necessária e suficiente para que o
sistema seja escalonável pela política EDF.  Já para o RM, a condição $U \leq
n(2^{1/n}-1)$ é somente suficiente para que o sistema seja escalonável.

Apesar de notórios avanços da área de escalonamento de tarefas, várias dificuldades
são ainda encontradas na aplicação dos resultados desta teoria em sistemas
computacionais modernos.  Por exemplo, a complexidade crescente dos programas de
software e dos dispositivos de hardware dificultam a determinação do custo
computacional máximo $C_i$. Podem também ser citadas outras dificuldades tais como o
gerenciamento do compartilhamento de recursos entre tarefas ou a consideração de
relações de precedência entre tarefas \cite{Burns01}.
%%% Lo Bello: PEAC





% Uma interrupção é usualmente definida como um evento que visa a alterar o fluxo de
% instruções executadas pelo processador \cite{Bovet05}.  Nos computadores modernos,
% as interrupções são gerenciadas por um dispositivo de hardware específico, o PIC ou
% o APIC (\ing{Advanced Programmable Interrupt Controller}) que é diretamente
% conectado ao processador. As linhas de interrupções (\ing{IRQ lines}) dos
% dispositivos de hardware são conectadas ao APIC que transmite as interrupções para o
% processador de acordo com uma política de prioridade predefinida.


\subsection{Chaveamento e Escalonamento}
\label{sec:tick}

Como já foi mencionado anteriormente, no modelo multi-programação e multi-usuário
dos computadores modernos, o Sistema Operacional deve prover a ilusão que vários
processos podem executar simultaneamente e/ou que vários usuários podem ter acesso
aos recursos de hardware ao mesmo tempo. Isto é obtido, nos sistemas de
compartilhamento do tempo, chaveando os processos entre si em fatias de tempo
curtas, da ordem de alguns milisegundos. A cada chaveamento entre dois processos, o
estado do processo executando é salvo para poder retomar sua execução ulteriormente.
Depois, o estado do novo processo é carregado nos registradores do processador.  Em
cada uma das suas ativações, um processo dispõe do processador para uma fatia de
tempo determinada, chamada quantum.  Para garantir o desempenho do sistema, o tempo
de execução de uma troca de contexto deve ser pequeno comparado ao quanta de cada
processo.

A tarefa de escolher qual dos processos prontos será o próximo a ser executado pelo
processador é realizada pelo escalonador (função \ing{schedule} do \kernell
Linux). Esta função é chamada diretamente por um processo que ``decide''
ceder o processador, por exemplo, quando ele está bloqueado a espera de um
dispositivo, ou indiretamente quando o processador detecta que o processo
executando deve ceder o processador.

A determinação da seqüência de execução dos processos utiliza o conceito de
prioridade. Num certo instante, escolher o próximo processo para ser executado passa
a ser calcular quais dos processos prontos tem a prioridade mais alta.  Para este
efeito, a teoria de escalonamento brevemente apresentada na seção \ref{sec:escal}
fornece uma diversidade importante de algoritmos que podem ser utilizados
\cite{Liu73, Audsley93}, de acordo com os atributos escolhido para definir a
heurística de cálculo da prioridade. No caso do Linux, cada processo tem uma
prioridade inicial. A medida que o tempo passa, esta prioridade é modificada
dinamicamente de acordo com o ciclo de vida do processo.  Por exemplo, a prioridade
de um processo tende a baixar a medida que o seu tempo de execução aumenta, de tal
forma que eventualmente, um processo esperando o processador o ganhará num tempo
finito. Para o mesmo efeito de justiça, a prioridade de um processo pronto aumenta a
medida que seu tempo de espera do processador aumenta.
