\documentclass{acm_proc_article-sp}

\usepackage{multirow}
\usepackage{rotating}
\usepackage{color}

\newcommand{\col}[1]{\textcolor{red}{#1}}
\newcommand{\cod}[1]{\hspace{0.1cm}\texttt{#1}}
\newcommand{\ing}[1]{\emph{#1}}
\newcommand{\kernel}{\ing{kernel}}
\newcommand{\kernell}{\ing{kernel} }
\newcommand{\preempt}{{Preempt-RT}}
\newcommand{\preemptt}{{Preempt-RT }}
\newcommand{\nanokernel}{\ing{nanokernel}}
\newcommand{\nanokernell}{\ing{nanokernel} }

\usepackage[font=normalsize, justification=Centering, singlelinecheck=false]{my_caption}
\usepackage[captionskip=1pt]{my_subfig}
%\renewcommand{\captionfont}{\sffamily\small\bfseries}


\begin{document}

% \usepackage{graphicx,url}

% \usepackage[brazil]{babel} \usepackage[latin1]{inputenc}

     
%\sloppy

\title{Evaluation of Interrupt Handling Timeliness\\in Real-Time Linux Operating Systems}

\numberofauthors{3}
\author{
\alignauthor
Paul Regnier\\%\titlenote{Dr.~Trovato insisted his name be first.}\\
       \affaddr{Distributed Systems Laboratory (LaSiD)}\\
       \affaddr{Computer Science Department (DCC)}\\
%       \affaddr{Campus de Ondina, 40170-110, Salvador-BA, Brazil}\\
       \affaddr{Federal University of Bahia}\\
       \email{pregnier@ufba.br}
%
\alignauthor
George Lima \\%\titlenote{Dr.~Trovato insisted his name be first.}\\
       \affaddr{Distributed Systems Laboratory (LaSiD)}\\
       \affaddr{Computer Science Department (DCC)}\\
%       \affaddr{Campus de Ondina, 40170-110, Salvador-BA, Brazil}\\
       \affaddr{Federal University of Bahia}\\
       \email{gmlima@ufba.br}
%
\alignauthor
Luciano Barreto \\%\titlenote{Dr.~Trovato insisted his name be first.}\\
       \affaddr{Distributed Systems Laboratory (LaSiD)}\\
       \affaddr{Computer Science Department (DCC)}\\
%       \affaddr{Campus de Ondina, 40170-110, Salvador-BA, Brazil}\\
       \affaddr{Federal University of Bahia}\\
       \email{lportoba@ufba.br}
}

\date{30 July 1999}


\maketitle
%\begin{multicols}{2}

\begin{abstract}
  Several real-time Linux extensions are available nowadays. Two of those extensions
  that have received special attention recently are Preempt-RT and Xenomai. This
  paper evaluates to what extent they provide deterministic guarantees when reacting
  to external events, an essential characteristic when it comes to real-time
  systems. To do so, we define a simple but effective experimental approach.
  Obtained results indicate that Preempt-RT is more prone to temporal variations
  than Xenomai when the system is subject to overload scenarios.
%  Several real-time Linux extensions can be found nowadays. Two of them have
%  received special attention recently, the patches Preempt-RT and Xenomai. This
%  paper evaluates to what extent they provide deterministic guarantees when reacting
%  to external events, an essential characteristic when it comes to real-time
%  systems. To do that we define a simple but effective experimental
%  approach. Obtained results indicate that Preempt-RT is more prone to temporal
%  variations than Xenomai when the system is subject to overload scenarios.
\end{abstract}
      

\section{Introduction}

Real-time systems encompasses a broad range of applications in telecommunications,
multimedia, industry, transportation, health etc. In all these scenarios, correctly
choosing a Real-Time Operating System (RTOS) is a fundamental design issue. Although
technological hardware advances are essential to the development of the IT industry,
some of these innovations may introduce undesirable impredictability to the
implementation of a RTOS. For example, cache memories, direct memory access (DMA),
co-processing, out-of-order execution, branch prediction and multi-core units may
introduce non-negligible sources of indeterminism \cite{Liu00, Pratt04}. Thus, the
construction of a general purpose operating system with a focus on timing
predictability remains a challenging research issue.

Although Linux is a popular and wide-spread OS, the standard Linux \kernell
\cite{Bovet05} fails to provide the timing guarantees required by critical real-time
systems \cite{Marchesotti06, Abeni02}. To circumvent this problem, several
approaches have been developed in order to increase the timing predictability of
Linux \cite{PreemptRT, Xenomai, Dozio03, rtLinux, Fry07, Calandrino06}. Such
diversity and the constant evolution in the OS design call for comparative studies
to assess the determinism degree offered by each platform in order to help real time
systems designers choosing the appropriate solution according to their needs.

This paper presents and compares two RT Linux \kernell patches \preemptt
(Linux$^{\mathbf{Prt}}$ \cite{PreemptRT} and Xenomai (Linux$^{\mathbf{Xen}}$)
\cite{Xenomai}, developed to increase the predictability of Linux. The main
contributions of this work are: (i) an evaluation framework based on simple software
and hardware COTS, and (ii) a report analysis of \preemptt and Xenomai performance
results obtained by our framework. Overall, these results show that
Linux$^{\mathbf{Xen}}$ Xen provides better critical timing guarantees than
Linux$^{\mathbf{Prt}}$.

The remainder of this paper is structured as follows. Section \ref{sec:compTemp}
discusses some factors of unpredictability in Linux and defines the metrics used in
our evaluation. Linux$^{\mathbf{Prt}}$ Linux$^{\mathbf{Xen}}$) are described in
Sections \ref{sec:preemptRT} and \ref{sec:xenomai}. We then present our evaluation
methodology in Section \ref{sec:metod} and experimental results in Section
\ref{cap:platEstud}.  Finally, Section \ref{sec:trabRel} briefly discusses related
work and Section \ref{sec:conc} concludes the paper.

\section{Comparison metrics}
\label{sec:compTemp}

% O método convencional utilizado para minimizar o impacto das interrupções sobre a
% execução dos processos consiste em dividir a execução do tratador de interrupção em
% duas partes. A primeira parte executa operações críticas de forma imediata e com as
% interrupções desabilitadas, o que constitui a \textbf{seção crítica} do
% tratador. Eventualmente, pode-se reabilitar as interrupções de modo a permitir
% preempção, tomando-se o cuidado de garantir o acesso controlado aos dados
% compartilhados através de \ing{locks}.  Na segunda parte, as operações não-críticas
% são possivelmente adiadas e executadas com as interrupções habilitadas.  No Linux,
% estas execuções postergadas são chamadas de \textbf{\emph{softirqs}}.

The conventional method used to minimize the impact of interruptions on the
implementation of processes is to divide the implementation of the interrupt
handlers into two parts. The first part, refered to as the \textbf{critical section}
of the handler, run critical operations immediatly, with interruptions disabled.
Eventually, one can enable interruptions during par of the critical section in order
to allow preemption. However, such implementation must rely on locks to ensure a
controlled access to shared data. The second part of the handler is dedicated to
non-critical operations. Its execution can be delayed, and normally happens with the
interruptions enabled. In Linux, such delayed executions are called
\textbf{\emph{softirqs}}.

\subsection{Interrupt latency}
\label{sec:latIRQ}

% Uma requisição de interrupção, ou simplesmente \textbf{interrupção}, do processador
% por um dispositivo de \ing{hardware} é assíncrona e pode acontecer em qualquer
% momento do ciclo de execução do processador. Em particular, tal requisição pode
% ocorrer enquanto a seção crítica do tratador de outra interrupção estiver
% executando, com as interrupções desabilitadas. Tal cenário pode provocar uma
% latência não determinística para a detecção da requisição de interrupção pelo
% processador.

An interrupt request, or simply \textbf{interrupt}, of the processor by a device
is tipically asynchronous and can happen at any time during the processor execution
cycle. In particular, such a request can occur while the critical section of another
interrupt handler is running, with interruptions disabled. This scenario may cause a
non deterministic latency for the detection of interrupt request by the processor.

% O tempo decorrido entre o instante no qual uma requisição de interrupção acontece e
% o início da execução do tratador associado é chamado de \textbf{latência de
%   interrupção}.  Esta grandeza foi contemplada como métrica para efeito de
% comparação das plataformas estudadas, pois caracteriza a capacidade do sistema para
% reagir a eventos externos.

The time interval between the instant in which an interrupt request happens and the
beginning of its associated handler execution is called \textbf{interrupt
  latency}. As it characterizes the system's capability to react to
external events, this quantity was regarded as a metric for the purpose of the comparison
of the studied platforms .

\subsection{Activation latency}
\label{sec:latAtiv}

% No \kernell Linux, logo após o término da seção crítica do tratador de interrupção,
% o \ing{softirq} correspondente está apto a executar. No entanto, entre o instante no
% qual a seção crítica termina e o instante no qual o \ing{softirq} começa a executar,
% outras interrupções podem acontecer, provocando um possível atraso na execução dos
% \ing{softirqs}.

In the \kernell Linux, just after the end of the interrupt handler critical section,
the associated \emph{softirq} becomes able to perform. However, between the instant
in which the critical section execution terminates and the instant in which deferred
\emph{softirqs} begins to execute, others interruptions may occur, causing a possible
delay in the \emph{softirqs} execution.

% Nas plataformas de tempo real, eventos de temporizadores ou de \ing{hardware} são
% utilizados para disparar tarefas, num modelo similar aos \ing{softirqs}. Tal tarefa,
% muitas vezes periódica, fica suspensa a espera de um evento.  Quando o evento
% ocorre, a requisição de interrupção associada aciona o tratador correspondente que,
% por sua vez, acorda a tarefa. O intervalo de tempo entre os instantes de ocorrência
% do evento e o início da execução da tarefa associada é chamada de \textbf{latência
%   de ativação}. Assim como no caso dos \ing{softirqs}, a latência de ativação pode
% ser aumentada pela ocorrência de interrupções. Além disso, a execução de outros
% \ing{softirqs} pode ser escalonada de acordo com alguma política (ex: FIFO,
% prioridade fixa), o que pode também gerar interferências na latência de ativação.
% Assim como a latência de interrupção, a latência de ativação caracteriza a
% capacidade de um sistema para reagir a eventos externos.

In real-time platforms, timeouts events or hardware events are used to trigger
tasks, in a similar manner as for \emph{softirqs}. Such a task, often periodic, is
suspended while waiting for some event. When this event occurs, the associated
interrupt request triggers the corresponding handler which, in turn, wakes the task
up. The time interval between the instant when the event occur and the beginning of
the execution of the associated task is called \textbf{activation latency}. As for
the \emph{softirqs}, the activation latency may be increased by the occurrence of
interruptions. Furthermore, the execution of other \emph{softirqs} may be scheduled
according to some policy (eg FIFO, fixed priority), which can also generate
interference in the activation latency. Like the interrupt latency, the activation
latency characterizes the capability of a system to react to external events. Thus,
this quantity was also regarded as a metric for the purpose of the comparison of the
studied platforms.

\section{Linux Preempt-RT}
\label{sec:preemptRT}

Linux$^{\mathbf{Prt}}$ \cite{McKenney05, Rostedt07} is a Linux real-time patch
originally developed by Ingo Molnar.  This patch makes the Linux kernel almost fully
preemptible by reengineering the use of locks inside the kernel.  As soon as a
process of highest priority is released, it can acquire the processor with minimal
latency, with no need to wait for the end of the execution of a lower priority
process, even if such a process is running in \kernell mode. Also, in order to limit
the unpredictability caused by shared resources, Linux$^{\mathbf{Prt}}$ provides
synchronization primitives that are able to use a priority inheritance protocol
\cite{Sha90}. Also, an specific implementation of high resolution timers
\cite{Kernel} allows the kernel to provide microsecond resolution. For instance,
Rosted et al \cite{Rostedt07, Siro07} obtained activation latencies of the order of
some tens of $\mu s$.

Linux$^{\mathbf{Prt}}$ creates specific kernel threads to handle both software and
hardware interrupts.  Upon an interrupt request, the associated handler masks
(acks/treats ?) the request, wakes up the associated thread and returns to the
interrupted code. This approach greatly reduces the execution latency of the
critical part of the interrupt handler. Another advantage of Linux$^{\mathbf{Prt}}$
is that several Linux software packages are available such as as C libraries and
programming environments.

% As an aside, the scheduling policy of interrupt threads may introduce some degree
% of unpredictability, which is also shown in Section xxx.  - isso é um resultado,
% não uma descrição

\section{Linux Xenomai}
\label{sec:xenomai}

Xenomai or Linux$^{\mathbf{Xen}}$ is a real-time Linux framework that encompasses an
OS kernel, APIs and a set of utilities. Linux$^{\mathbf{Xen}}$ uses an interrupt
indirection layer, also called \nanokernel, to isolate real-time tasks from Linux
processes. In this approach, when an interrupt occurs, the \nanokernel forwards the
request either to a real-time task or to a conventional Linux process. In the first
case, the interrupt handler runs immediately. In the second case, the request is
enqueued and further delivered to Linux when there are no real-time tasks. Whenever
the Linux \kernell must disable the interrupts, the \nanokernell just makes the
Linux \kernell believe that interrupts are disabled. Still, the \nanokernell keeps
intercepting any hardware interrupts. Any interrupts directed to a real-time task
are treated immediatly, while interrupts targeted to Linux are enqueued until the
Linux \kernell enable them again.

The Linux$^{\mathbf{Xen}}$ \nanokernel is based on resource virtualization layer
called Adeos (Adaptative Domain Environment for Operating Systems)
\cite{Yaghmour01}. Adeos eases hardware sharing and provides a small API which is
architecture independent. Adeos relies on two basic concepts: domains and
hierarchical interrupt pipelines. A domain defines an isolated execution
environment, in which one can run programs or even a complete operating system. The
hierarquical interrupt pipeline, called \textbf{ipipe}, performs interrupt delivery
across different domains. When a domain is registered, it is stored on a specific
position in the ipipe according to its timing requirements. The interrupt
indirection mechanism handles hierarchical interrupt delivery following the priority
associated to each domain.

Real-time services in Linux$^{\mathbf{Xen}}$ correspond to the highest priority
domain in the ipipe, which is called the primary domain. The secondary domain refers
to the Linux \kernell, from which common Linux software libraries are available. At
this level, however, timing guarantees are weak due to blocking system calls.

% A implementação dos serviços de tempo real em modo usuário se baseia no mecanismo
% de herança de prioridade e num domínio intermediário, chamado ``escudo de
% interrupção'' que garante as seguintes propriedades: (i) a política de prioridade
% utilizada para as tarefas de tempo real, e para o escalonador associado, é comum
% aos dois domínios; e (ii) as interrupções de \ing{hardware} não podem impedir a
% execução de uma tarefa prioritária enquanto ela estiver no domínio secundário.


\section{Experimental methodology}
\label{sec:metod}

\begin{figure*}[t]
  \centering {\scalebox{1}{\input{fig/dispExp.pstex_t}}}
  \caption{Interrupt and activation latencies measurement at station $E_M$}
  \label{fig:dispExp}
\end{figure*}

In general, performing accurate time measurements at the interrupt handler level is
not simple and may require the use external devices such as oscilloscopes or other
computers. In fact, the exact instant at which an interrupt request occurs is
difficult to be determined since this is an asynchronous event which can be
triggered by any hardware device. Nevertheless, since the objective of this work is
to characterize and compare the degree of predictability in operating systems
platforms, simple experiment set-up can be used.  In other words, we are interested
in measuring approximate values of latencies for different real-time OS under
similar load scenarios.
 
To compare the OS platforms two experiments were set up, both of which use only
computer stations connected to each other by standard communication devices. These
two experiments, described in Sections \ref{sec:exp1} and \ref{sec:exp2}, are to
measure interrupt and activation latencies with and without overload scenarios.
These latencies are denoted $L_{irq}$ and $L_{ativ}$, respectively. The
procedure to generate overload scenarios is given in Section \ref{sec:carga}.

\subsection{First experiment}
\label{sec:exp1}

Figure \ref{fig:config} illustrates how the first experiment was set up. As can be
seen, there are three stations, $E_T$, $E_L$ and $E_M$. Two distinct Ethernet
network devices, $eth_0$ and $eth_1$, are used to connect $E_T$ to $E_M$ and $E_L$
to $E_M$, respectively. The role of $E_T$ is to \emph{trigger} events at the
parallel port of $E_M$. Such events should be timely handled by $E_M$, the station
whose latencies are \emph{measured}.  $E_L$ is the \emph{load} station, used to
create overload scenarios on $E_M$ via $eth_1$.

\begin{figure}[tbh]
  \centering {\scalebox{1}{\input{fig/config.pstex_t}}}
  \caption{First experiment set up}
  \label{fig:config}
\end{figure}

The activities handled by $E_M$ are illustrated in Figure \ref{fig:dispExp}. The
following sequence of events occurs:

\begin{enumerate}
\item $E_T$ sends Ethernet frames to $E_M$, which are received through its $eth_0$
  device. Upon the receiving of each frame the device $eth_0$ issues an interrupt
  request in $E_M$. In turn, the associated interrupt handler ($T_{eth_0}$) preempts
  the application that is executing on $E_M$.

\item $T_{eth_0}$ is reduced to its minimum. It only sets the Parallel Port
  Interrupt Request (PP-IRQ) line and saves the instant $t_1$ in memory. Note that
  $t_1$ is the local time on $E_M$ and is read just after the arrival of an Ethernet
  frame at $eth_0$.

\item Upon the detection of the PP-IRQ, its handler $T_{PP}$ preempts the
  application on $E_M$.  Then $T_{PP}$ saves the instant $t_2$ and wakes up task
  $\tau$. This second time instant is the local time value at $E_M$ just after the
  start of $T_{PP}$.
  
\item When task $\tau$ wakes up, it saves instant $t_3$ in memory and then is
  suspended until the next PP interrupt. Thus, $t_3$ is the time instant at which
  $\tau$ starts executing.
\end{enumerate}

The measured values of $L_{irq}$ and $L_{ativ}$ were transfered from main memory to
a file system in $E_M$ by a user process using a FIFO channel. The assigned priority
of the user process was lower than the priority of interrupt handlers.  Also, this
data transferring procedure were sufficiently rare events (20 per second). This data
transferring scheme was to prevent possible interference in the measured values.

Both latencies can be computed by the described procedure as $L_{irq} = t_2 - t_1$
and $L_{ativ} = t_3 - t_2$, as depicted in Figure \ref{fig:dispExp}. It is worth
noticing that the measurements are realized by the same station that is responsible
for managing real-time activities. Indeed, station $E_M$ waits for the asynchronous
arrival of an Ethernet frame at $eth_0$ to trigger the corresponding parallel port
interrupt so that measurements can be carried out. This dependence between external
and internal events may compromise the measurements. In order to evaluate to what
extent such a procedure interfere in the measurements, a second experiment was set
up.

\begin{figure*}[t!]
  \centering {\scalebox{1}{\input{fig/dispExp2.pstex_t}}}
  \caption{Interrupt and activation latencies measurement at station $E_M$}
  \label{fig:dispExp2}
\end{figure*}
\vspace{1cm}

\subsection{Second experiment}
\label{sec:exp2}

The same three stations $E_M$, $E_T$ and $E_L$ are used in this new experiment
set-up. Station $E_L$ is configured as before while the set up of the other
two stations are modified, as illustrated by Figure \ref{fig:config2}.

\begin{figure}[htb]
  \centering {\scalebox{1}{\input{fig/config2.pstex_t}}}
  \caption{Second experimental set up}
  \label{fig:config2}
\end{figure}

Similar to the first experiment, the values of $L_{irq}$ and $L_{ativ}$ to be
measured correspond to real-time activities executed in $E_M$. However, the
measurements are carried out by $E_T$ instead. The measurement procedure makes use
of the parallel port that connects $E_T$ and $E_M$, as can be seen from the figure.
The device $eth_0$ is no longer necessary. In other words, PP interrupt at $E_M$,
triggered by $E_T$, are now issued via this parallel port. Station $E_M$ handles
such PP interrupt requests, waking up a real-time task $\tau$ similarly to the
previous experiment. Note that the measurements could not be carried out by $E_M$
unless station local clocks were synchronized to each other.

Figure \ref{fig:dispExp2} summarizes the sequence of events that occur in $E_M$ and
$E_T$ and make up the second measurement procedure:

\begin{enumerate}
\item Station $E_T$ triggers an interrupt request on the PP-IRQ line of station
  $E_M$ and saves instant $t_1$ in memory. This time instant is the local clock of
  $E_T$ just after the interrupt was requested.
 
\item An interrupt is issued at the parallel port of $E_M$. Its handler
  $T_{PP}(E_M)$ is activated, causing the preemption of the application running on
  $E_M$.

\item The handler $T_{PP}$ of $E_M$ triggers an interrupt request on the PP-IRQ line
  of station $E_T$ and wakes up task $\tau$.
  
\item The PP interrupt handler $T_{PP}(E_T)$ of $E_T$ saves time $t_2$ in memory.
  This time instant corresponds to the value of the local clock of $E_T$ just after
  the start of $T_{PP}(E_T)$.
  
\item Task $\tau$ wakes up in $E_M$ and triggers a new interrupt request on the
  PP-IRQ line of station $E_T$.
  
\item The handler $T_{PP}(E_T)$ is activated to deal with this second interrupt
  request, saving the current value of its local clock $t_3$. This instant
  corresponds to the time at which $E_T$ is informed about the activation of $\tau$.
\end{enumerate}


As can be seen from Figure \ref{fig:dispExp2}, the described measurement procedure
must take into account the interrupt latency $\delta$ in $E_T$. Indeed, differently
from the first experiment, now $L_{irq} = t_2 - t_1 - \delta$. However,
$L_{ativ}$ cannot be accurately measured anymore.  Indeed, the interruption issued
by $\tau$ may take place before or after $t_2$, which makes the value of $t_3 - t_2$
too imprecise to be taken as a measurement of $L_{ativ}$.

The value of $\delta$ can be estimated as follows by carring out the first
experiment, but without using station $E_L$. For example, using
Linux$^{\mathbf{Xen}}$ running in single mode with minimal load, the estimated value
of $\delta$ can be taken as the average value observed in the measurements,
$\bar{\delta} = 9 \mu s$ with standard deviation $0.1 \mu s$. Once this estimation
is derived, the operating system patch of $E_T$ must be fixed so that $\bar{\delta}$
can be used as an accurate estimation of $\delta$ during the experiments.

The measurements obtained by the described procedure were transfered from memory to
a file system in $E_T$ using the same data transferring scheme used in the first
experiment.

\subsection{I/O, processing and interrupt loads}
\label{sec:carga}

The experiments were carried out with and without overload scenarios in station
$E_M$. When overload were not considered, $E_M$ was set up with its \kernell in
single mode and with minimum activities, i.e. both $E_L$ and no other process in
$E_M$ generate extra load. As will be seen, in general, the analyzed real-time
patches present high levels of predictability under this situation.

As for overload scenarios, the station $E_M$ was stressed by two different types of
loads, triggered by internal and external events. Both types of loads were started a
few seconds later than the begining of the measurements.  As will be seen, under
such overload scenarios, it was possible to assess to what extent the analyzed
real-time patches can provide predictability.

The internal events used to cause processing and I/O loads on $E_M$ was due to
executing the following instructions:

%\begin{table}[h]
%\caption{Processor load instruction set}
%\vspace{ 4pt }
% \setstretch{0.94}
%\begin{center}
  \framebox[0.47\textwidth]{%
    \begin{minipage}{0.84\linewidth}
      \vspace{2pt } \scriptsize{%
        \texttt{while "true"; do\\
          \rule{0.3cm}{0pt} dd if=/dev/hda2 of=/dev/null bs=1M count=1000\\
          \rule{0.3cm}{0pt} find / -name "*.c" | xargs egrep include\\
          \rule{0.3cm}{0pt} tar -cjf /tmp/root.tbz2 /usr/src/linux-xenomai\\
          \rule{0.3cm}{0pt}  cd /usr/src/linux-preempt; make clean; make\\
          done \vspace{2pt } %\nolinebreak %\raisebox{10mm}{\rule{0cm}{0.01cm}}
        }}
    \end{minipage}
  }
%\end{center}
%\end{table}
%\vspace{ 4pt }

% \setstretch{1}

The external events used to overload $E_M$ were due to the arriving of 64 byte UDP
packages at $eth_1$ sent by station $E_L$. Station $E_M$ was configured as a server
while $E_L$ was the client. The package sending rate was set to $200 kHz$, the
maximum allowed by the network bandwidth. In other words, more than $100,000$
interrupt requests per second were issued at $eth_1$. This device made use of the $E_M$
IRQ line $18$, whose priority is lower than the priority of the PP-IRQ line. Thus,
in an ideal situation, one would expect that receiving packages from $eth_1$ would
not interfere in the processing of PP-IRQ related events.

\section{Evaluation results}
\label{cap:platEstud}

The experiments were conducted on three Pentium 4 computers with $2.6 GHz$
processors and $512 MB$ RAM memory. Three operating system platforms were analyzed:

\begin{description}
\item[$\bullet \;$ Linux$^{\mathbf{Std}}$:] Linux standard - \kernell version 2.6.23.9
  (\ing{low-latency} option);
\item[$\bullet \;$ Linux$^{\mathbf{Prt}}$:] Linux with \ing{patch}
  \preemptt (rt12) - \kernell version 2.6.23.9;
\item[$\bullet \;$ Linux$^{\mathbf{Xen}}$:] Linux with \ing{patch} Xenomai - version
  2.4-rc5 - \kernell version 2.6.19.7.
\end{description}

Linux$^{\mathbf{Std}}$ was considered for the sake of illustration only since it is
not suitable to deal with real-time applications. It also may serve as a comparison
reference, against with one compare Linux$^{\mathbf{Prt}}$ and
Linux$^{\mathbf{Xen}}$.  Latencies $L_{irq}$ and $L_{ativ}$ were measured using the
Time Stamp Counter (TSC), which provided a precision of less than $30 ns$ (88
cycles), experimentally verified.  As mentioned earlier, station $E_T$ was used to
trigger $20Hz$ events at station $E_M$.  The measured data were the result of
running the experiments for 10 minute for each considered set up and
platform.

We first present in Section \ref{sec:results-1} the results from experiment 1
regarding the analyzed platforms. Then, in Section \ref{sec:results-2}, we analyze
the procedure suggested by second experiment. As will be seen, the results obtained
by the second experiment validate the first one.

Experimental results are presented through graphs in which the horizontal axis
represents the instant of observation ranging from 0 to 60 seconds and the vertical
axis represents the measured latencies in $\mu s$ (one can multiplied by $2.6 10^3$
such values to obtain the corresponding number of TSC cycles). Although each
experiment was run for ten minutes, the graphs plot only a window of $60s$ as such
interval size is sufficient to illustrate the timing behavior of each platform.
During this one-minute interval, the total number of events is 1200 as the arrival
frequency of Ethernet frames at the $eth_0$ network device of station $E_M$ is $20
Hz$.

Below each graph the following values are given: Mean (M), Standard Deviation
(SD), minimum (Mn) and maximum (Mx). These numbers were obtained considering the
duration of ten minutes of each experiment. Values outside the vertical axis range
are represented by a triangle near the maximum value.

\subsection{First Setup Results}
\label{sec:results-1}

For the sake of illustration, we first discuss the results regarding
Linux$^\mathbf{Std}$.  Then, we present the measurements obtained for the other
platforms.

\subsubsection{Linux$^\mathbf{Std}$}

As can be seen from Figure \ref{fig:latIrq-std}, the obtained values without load
show that interrupt handling in Linux is reasonably efficient. As it will be seen
shortly, these values are very close to some real-time OS platforms.  However, both
$L_{irq}$ and $L_{ativ}$ vary significantly in the presence of overload, as
expected.  In particular, the obtained values of $L_{ativ}$ in overload scenarios
confirm that Linux$^\mathbf{Std}$ is not suitable to support real-time systems.

\newpage
\begin{figure*}[t!]%
% \centering
 \subfloat[\small{\textbf{Linux$^{\mathbf{Std}}$ - Interrupt latency - without load} \newline
 \vskip 1mm M:     8.9, SD:     0.3,  Mn:     8.7, Mx:    18.4 }]{%
   \label{fig:ker23Sem}%
   {\scalebox{0.67}{\input{fig/ker23Sem}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Std}}$ - Interrupt latency - with loads} \newline
 \vskip 1mm M:    10.4, SD:     1.9,  Mn:     8.8, Mx:    67.7 }]{%
   \label{fig:ker23Tot}%
   {\scalebox{0.67}{\input{fig/ker23Tot}}}}%
   
 \subfloat[\small{\textbf{Linux$^{\mathbf{Std}}$ - Activation latency - without load} \newline
 \vskip 1mm M:     4.6, SD:     0.4,  Mn:     4.4, Mx:    16.2}]{%
   \label{fig:ker23SemSched}%
   {\scalebox{0.67}{\input{fig/ker23SemSched}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Std}}$ - Activation latency - with loads}\newline
 \vskip 1mm M:    37.3, SD:    48.2,  Mn:     4.6, Mx:   617.5}]{%
   \label{fig:ker23TotSched}%
   {\scalebox{0.67}{\input{fig/ker23TotSched}}}}
   
 \caption[Linux$^{\mathbf{Std}}$ latencies]{Linux$^{\mathbf{Std}}$ latencies with
   $20 Hz$ PP-IRQ trigging frequency using $eth_0$ interrupt handler.}
 \label{fig:latIrq-std}%
\end{figure*}


\subsubsection{Linux$^\mathbf{Prt}$ and Linux$^\mathbf{Xen}$}

Figures \ref{fig:latIrq} and \ref{fig:latAtiv} plot the values of $L_{irq}$ and
$L_{ativ}$, respectively.  Six graphs are shown in each figure. As can be seen,
there are two versions of Linux$^{\mathbf{Prt}}$.  Two of them, Figures
\ref{fig:preSemND} and \ref{fig:preTotND}, correspond to setting the option
\cod{IRQF\_NODELAY} at the initialization time of IRQ line. With this option,
interrupt handling of that line is implemented without treads. Figures
\ref{fig:preSem} and \ref{fig:preTot} correspond to the usual threaded
implementation in Linux$^{\mathbf{Prt}}$.

As for the interrupt latencies, Linux$^{\mathbf{Xen}}$ clearly shows higher
predictability when compared to the other platforms. Under overload scenarios, this
behavior is evident as it can be noticed by the lower mean and standard deviation
values. In order to explain the behavior of Linux$^{\mathbf{Prt}}$, some aspects
need to be explained.  First, as can be observed, using option \cod{IRQF\_NODELAY}
in requesting the PP-IRQ line, the behavior of Linux$^{\mathbf{Prt}}$ turns to be
similar to Linux$^{\mathbf{Std}}$ although Linux$^{\mathbf{Prt}}$ exhibits better
results. On the other hand, using threads for interrupt handling increases the
interrupt latency due to an extra context-switching overhead.  Also, a significantly
higher variability on the latency values can be observed when the system is
overloaded.  This fact can be explained by the execution delay of the handler.
Indeed, between the instant at which the $T_{PP}$ handler issues the interrupt and
the instant at which the IRQ thread actually wakes up, several interrupts may occur.
In such a scenario, the execution of associated interrupt handlers may delay the
execution of $T_{PP}$.

\begin{figure*}[t!]%
 \centering
 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - without load} \newline
 \vskip 1mm M: 21.5, SD: 1.7, Mn: 20.3, Mx: 45.1 }]{%
   \label{fig:preSem}%
   {\scalebox{0.67}{\input{fig/preSem}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - with loads} \newline
 \vskip 1mm M: 58.5, SD: 26.4, Mn: 17.2, Mx: 245.9}]{%
   \label{fig:preTot}%
   {\scalebox{0.67}{\input{fig/preTot}}}}%

 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - without load, option
     \cod{IRQF\_NODELAY}} \newline
   \vskip 1mm M: 8.9, SD: 0.2, Mn: 8.8, Mx: 16.7}]{%
   \label{fig:preSemND}%
   {\scalebox{0.67}{\input{fig/preSemND}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - with loads, option
     \cod{IRQF\_NODELAY}} \newline
   \vskip 1mm M: 10.6, SD: 1.6, Mn: 8.9, Mx: 35.8}]{%
   \label{fig:preTotND}%
   {\scalebox{0.67}{\input{fig/preTotND}}}}

 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - without load} \newline
 \vskip 1mm M: 9.0, SD: 0.1, Mn: 8.8, Mx: 11.1}]{%
   \label{fig:xenSem}%
   {\scalebox{0.67}{\input{fig/xenSem}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - with loads} \newline
 \vskip 1mm M: 10.2, SD: 0.1, Mn: 8.8, Mx: 20.8}]{%
   \label{fig:xenTot}%
   {\scalebox{0.67}{\input{fig/xenTot}}}}%

 \caption[Interrupt latencies]{Interrupt latencies  with $20 Hz$ PP-IRQ trigging
   frequency using $eth_0$ interrupt handler.}
 \label{fig:latIrq}%
\end{figure*}

Figure \ref{fig:latAtiv} shows activation latencies with and without load.  As can
be seen, Linux$^{\mathbf{Prt}}$ and Linux$^{\mathbf{Xen}}$ have values of latencies
within the expected standard. It is worth noting the behavior of these systems with
load. Although the average value found for Linux$^{\mathbf{Xen}}$ ($8,7 \mu s$) is
superior to Linux$^{\mathbf{Prt}}$ ($3,8 \mu s$), the standard deviation is
significantly lower in favor of Linux$^{\mathbf{Xen}}$. In fact, this is a desirable
feature in real-time critical systems. Additionally, for such systems, it is
desirable that the worst case execution time be as close as possible to the average
case execution time.
       
\begin{figure*}[t!]%
 \centering
 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - without load} \newline
 \vskip 1mm M: 2.1, SD: 0.2, Mn: 1.2, Mx: 9.4}]{%
   \label{fig:preSemSched}%
   {\scalebox{0.67}{\input{fig/preSemSched}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - with loads} \newline
 \vskip 1mm M: 3.8, SD: 2.8, Mn: 1.1, Mx: 27.4}]{%
   \label{fig:preTotSched}%
   {\scalebox{0.67}{\input{fig/preTotSched}}}}

 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - without load, option
     \cod{IRQF\_NODELAY}} \newline
 \vskip 1mm M:     5.3, SD:     0.3,  Mn:     5.0, Mx:    13.1}]{%
   \label{fig:preSemSchedND}%
   {\scalebox{0.67}{\input{fig/preSemSchedND}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - with loads, option
     \cod{IRQF\_NODELAY}} \newline
 \vskip 1mm M:     8.0, SD:     2.0,  Mn:     5.2, Mx:    31.0}]{%
   \label{fig:preTotSchedND}%
   {\scalebox{0.67}{\input{fig/preTotSchedND}}}}
 
 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - without load} \newline
   \vskip 1mm M: 2.1, SD: 0.5, Mn: 1.8, Mx: 8.4}]{%
   \label{fig:xenSemSched}%
   {\scalebox{0.67}{\input{fig/xenSemSched}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - with loads} \newline
   \vskip 1mm M: 8.7, SD: 0.3, Mn: 1.8, Mx: 18.7}]{%
   \label{fig:xenTotSched}%
   {\scalebox{0.67}{\input{fig/xenTotSched}}}}

 \caption[]{Activation latencies with $20 Hz$ PP-IRQ trigging
   frequency using $eth_0$ interrupt handler.}
 \label{fig:latAtiv}%
\end{figure*}

It is also interesting to compare the behavior of Linux$^{\mathbf{Prt}}$ without
using the implementation of interrupt threads shown in Figure\ref{fig:latAtiv}.
Although activation latencies without load are acceptable when compared to
Linux$^{\mathbf{Prt}}$, the results obtained with system load indicate a slightly
less predictable behavior than Linux$^{\mathbf{Xen}}$.


\subsection{Second Setup results}
\label{sec:results-2}

As expected, the results obtained by the second experiment were similar to those
described in the previous section. In order to avoid repeating the observed patterns
of analyzed platforms, we summarized the results in Table \ref{tab:dataSetUp1} for the first
 and Table \ref{tab:dataSetUp2} for the second experiment.

 For illustrating the differences between the second and the first experiment, we
 chose to show the results regarding Linux$^{\mathbf{Xen}}$ only rot two main reasons:
 (i) first, this platform has shown better real-time capability; and (ii) station $E_T$ uses
Linux$^{\mathbf{Xen}}$ in order to obtain reliable measurements.

The experimental results for the second experiment (see Section
\ref{sec:exp2}) are shown in Figure \ref{fig:latIrqAtiv2}.

\subsubsection{Interrupt latencies}
\label{sec:latIrq}

Figure \ref{fig:latIrqAtiv2} shows Linux$^{\mathbf{Xen}}$ interrupt latencies measured, with
and without system load.

The first graph \ref{fig:xenSem2} deserve a special attention.  As expected, the
measurements obtained for the interrupt latency equals the double of those obtained
with the first set up. As graph \ref{fig:xenSem2} corresponds to a perfectly
symmetric scenario, where both station $E_M$ and $E_T$ executes
Linux$^{\mathbf{Xen}}$ without any load, the interrupt latency at $E_M$ and $E_M$
are equals. Hence, we can safely deduce that $\delta \approx 9.1$, as the standard
deviation of this scenario is leather than $0.3$.  Recall from Section
\ref{sec:exp2} and Figure \ref{fig:dispExp2}, that $\delta$ is the extra latency
introduced by the second set up, corresponding to the interrupt latency of $E_T$.

Once subtracted $\delta$ from results of graph \ref{fig:xenTot2}, the results
obtained for the interrupt latency with overload are very similar to those obtained
with the first set up. A noticeable difference is the increase of the standard
deviation which was multiplied by 15. However, as a similar increase is observed for
small values of SD in the presence of overload, we believe that it may be caused by
variation of the response time of the interrupt device (APIC).
 
\subsubsection{Activation latencies}

Figure \ref{fig:latIrqAtiv2} shows Linux$^{\mathbf{Xen}}$ activation latencies
measured, with and without load of the system.

Once again, the graph \ref{fig:xenSemSched2} obtained without load deserve a special
attention.  Indeed, the mean value obtained here for the activation latency is much
higher ($6.5$) than this obtained with the first experimental set up ($2.1$).  This
can be explain, as illustrated by Figure \ref{fig:dispExp3}, by the following
arguments. The time interval elapsed between the triggering of the first
interrupt by $IRQ(T_{PP})$ and the second interrupt by $IRQ(\tau)$ is leather
than $\delta$. Hence, when the interrupt request triggered by $E_M$ arrives
at $E_T$, the first interrupt is still pending. Therefore, $E_T$ handles
both interrupt in a row, and the latency measured is the minimum latency $L_{min}$
that can separate the execution of two consecutive interrupt handler.

\begin{figure*}[t!]
 \centering {\scalebox{1}{\input{fig/dispExp3.pstex_t}}}
 \caption{Minimal activation latency - The first and second interrupt triggered at
  $E_T$ by $E_M$ occurs in a time interval leather than $\delta$.}
 \label{fig:dispExp3}
\end{figure*}

In order to verify such an interpretation, we slightly modified the second experiment,
introducing a constant delay of $10 \mu s$ between the triggering of  $IRQ(T_{PP})$
$IRQ(\tau)$. Doing so, we guaranteed that $E_T$ had time to handle the first interrupt
before the arrival of the second. The obtained results shown that the mean measured latency
was of $12.3$. Thus, subtracting the $10 \mu s$ delay, we corroborate the measurements 
obtained with set up 1 with a precision better than half  $\mu s$.

\begin{figure*}[t!]%
 \centering

 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - Interrupt latency - without load} \newline
 \vskip 1mm M:    18.2, SD:     0.3,  Mn:     9.1, Mx:    23.1 }]{%
   \label{fig:xenSem2}%
   {\scalebox{0.67}{\input{fig2/xenSem}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - Interrupt latency - with loads} \newline
 \vskip 1mm M:    20.7, SD:     1.5,  Mn:     9.1, Mx:    30.0 }]{%
   \label{fig:xenTot2}%
   {\scalebox{0.67}{\input{fig2/xenTot}}}}%

 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - Activation latency - without load} \newline
 \vskip 1mm M:     6.5, SD:     0.5,  Mn:     5.0, Mx:    13.2 }]{%
   \label{fig:xenSemSched2}%
   {\scalebox{0.67}{\input{fig2/xenSemSched}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - Activation latency - with loads} \newline
 \vskip 1mm M:    11.1, SD:     3.0,  Mn:     5.0, Mx:    25.7 }]{%
   \label{fig:xenTotSched2}%
   {\scalebox{0.67}{\input{fig2/xenTotSched}}}}

 \caption[Activation latencies]{Xenomai latencies on $E_M$ with $20 Hz$ PP-IRQ trigging
   frequency.}
 \label{fig:latIrqAtiv2}%
\end{figure*}

\newpage
\newcommand{\tabSize}{@{\hspace{0.cm}} r @{\hspace{0.1cm}}}%{p{0.6cm}}
\newcommand{\cspc}{\rule{0.14cm}{0cm}}
\newcommand{\cspcc}{\rule{0cm}{0cm}}
\begin{table*}[t]
\centering
\caption{Latency results for Linux$^{\mathbf{Std}}$, Linux$^{\mathbf{Prt}}$,
Linux$^{\mathbf{PrtND}}$ (option IRQF\_NODELAY) Linux$^{\mathbf{Xen}}$ using
 Set Up 1 \vspace{0.4cm}}
 \label{tab:dataSetUp1}
\begin{tabular}{@{} p{0.9cm}||\tabSize|\tabSize|\tabSize|\tabSize||\tabSize|\tabSize|
    \tabSize|\tabSize||\tabSize|\tabSize|\tabSize|\tabSize||\tabSize|\tabSize|\tabSize|\tabSize||}
\cline{2-17}
\multirow{2}{*}{}
&
\multicolumn{4}{|c||}{\rule{0cm}{0.4cm}Linux$^{\mathbf{Std}}$}&
\multicolumn{4}{|c||}{Linux$^{\mathbf{Prt}}$}&
\multicolumn{4}{|c||}{Linux$^{\mathbf{PrtND}}$}&
\multicolumn{4}{|c||}{Linux$^{\mathbf{Xen}}$}\\

\cline{2-17}
\rule{0cm}{0.3cm} Load &
\multicolumn{2}{|c|}{\rule{0cm}{0.3cm}No}&
\multicolumn{2}{|c||}{Yes}&
\multicolumn{2}{|c|}{No}&
\multicolumn{2}{|c||}{Yes}&
\multicolumn{2}{|c|}{No}&
\multicolumn{2}{|c||}{Yes}&
\multicolumn{2}{|c|}{No}&
\multicolumn{2}{|c||}{Yes}\\
\hline

&
\rule{0cm}{0.3cm}\cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc \\
\hline
\rule{0cm}{0.35cm} Mean &
  8.9 &  4.6 & 10.4 & 37.3 & 21.5 &  2.1 & 58.5 & 3.8 &
 8.9 &  5.3 & 10.6 &   8.0 &   9.0 &  2.1 & 10.2 & 8.7\\
\hline
\rule{0cm}{0.35cm} SD &
 0.3 &  0.4 &  1.9 & 48.2 &  1.7 &  0.2 & 26.4 & 2.8 &
 0.2 &  0.3 &  1.6 & 2.0 &   0.1 &   0.5 &   0.1 & 0.3\\
\hline
\rule{0cm}{0.35cm} Min &
  8.7 &  4.4 &  8.8 &  4.6 & 20.3 &  1.2 & 17.2 & 1.1 &
  8.8 &  5.0 &  8.9 &  5.2 &   8.8 &  1.8 &   8.8 & 1.8\\
\hline
\rule{0cm}{0.35cm} Max &
18.4 & 16.2 & 67.7 & 617. & 45.1 & 9.4 & 245. & 27.4 &
16.7 & 13.1 & 35.8 & 31.0 & 11.1 & 8.4 & 20.8 & 18.7\\
\hline
\end{tabular}
\end{table*}

\begin{table*}[t!]
\centering
\caption{Latency results for Linux$^{\mathbf{Std}}$, Linux$^{\mathbf{Prt}}$,
Linux$^{\mathbf{PrtND}}$ (option IRQF\_NODELAY) Linux$^{\mathbf{Xen}}$ using
 Set Up 2 \vspace{0.4cm}}
 \label{tab:dataSetUp2}
\begin{tabular}{@{} p{0.9cm}||\tabSize|\tabSize|\tabSize|\tabSize||\tabSize|\tabSize|
    \tabSize|\tabSize||\tabSize|\tabSize|\tabSize|\tabSize||\tabSize|\tabSize|\tabSize|\tabSize||}
\cline{2-17}
\multirow{2}{*}{}
&
\multicolumn{4}{|c||}{\rule{0cm}{0.4cm}Linux$^{\mathbf{Std}}$}&
\multicolumn{4}{|c||}{Linux$^{\mathbf{Prt}}$}&
\multicolumn{4}{|c||}{Linux$^{\mathbf{PrtND}}$}&
\multicolumn{4}{|c||}{Linux$^{\mathbf{Xen}}$}\\

\cline{2-17}
\rule{0cm}{0.3cm} Load &
\multicolumn{2}{|c|}{\rule{0cm}{0.3cm}No}&
\multicolumn{2}{|c||}{Yes}&
\multicolumn{2}{|c|}{No}&
\multicolumn{2}{|c||}{Yes}&
\multicolumn{2}{|c|}{No}&
\multicolumn{2}{|c||}{Yes}&
\multicolumn{2}{|c|}{No}&
\multicolumn{2}{|c||}{Yes}\\
\hline

&
\rule{0cm}{0.3cm}\cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc &
 \cspc$L_{irq}$\cspcc & \cspc$L_{ativ}$\cspcc \\
\hline
\rule{0cm}{0.35cm} Mean &
 18.0 &  5.7 & 21.8 & 23.3 & 19.4 &  6.1 & 41.9 & 10.1 &
 18.1 &  6.8 & 21.2 & 15.6 & 18.2 &  6.5 & 20.7 & 11.1\\
\hline
\rule{0cm}{0.35cm} SD &
  0.3 &  0.6 &  3.2 & 28.5 & 0.5 &  0.6 & 18.9 & 3.7 &
  0.3 &  0.4 &  2.4 &  6.6 &  0.3 &  0.5 &   1.5 & 3.0\\
\hline
\rule{0cm}{0.35cm} Min &
  9.6 &  5.0 &  9.8 &  5.0 &  9.8 &  5.0 &  9.7 & 5.0 &
  9.1 &  5.1 &  9.1 &  5.0 &  9.1 &  5.0 &  9.1 & 5.0\\
\hline
\rule{0cm}{0.35cm} Max &
 27.3 & 15.6 & 80.6 & 623. & 42.2 & 17.3 & 324. & 70.5 &
 25.5 & 15.4 & 50.5 & 107. &  23.1 & 13.2 & 30.0 & 25.7\\
\hline
\end{tabular}
\end{table*}

\subsubsection{Discussion}

Comparison of set up 1 and 2

The only other noticeable difference is related to Linux $^{\mathbf{Prt}}$ whose
interrupt latencies are smaller in the second setup. We believe this result is due
to the thread implementation model, which reacts faster to hardware interrupt
requests than software interrupt requests. Similar to the first experiment setup, we
unset \cod{IRQF\_NODELAY} to disable parallel port IRQ threading and observed that
the behavior of Linux$^{\mathbf{Prt}}$ was similar to Linux$^{\mathbf{Std}}$.

(ESSE PARÀGRAFO NÂO DIZ GRANDE COISA)
As a first conclusion, we emphasize that results obtained with the second
experimental set up are similar to those obtained with the first experimental
set up, which motivates the use of the first experiment set up for
future comparison works.

(NÂO ENTENDI ESSE PARÀGRAFO)
Except for this increase of the latencies by the $\delta$ interrupt latency at
station $E_T$, and its related standard deviation, results obtained with the second
experiment are similar to those obtained in the first experimental set up.



\section{Related work}
\label{sec:trabRel}

Some experimental results comparing Linux$^{\mathbf{Prt}}$ and
Linux$^{\mathbf{Std}}$ are presented in \cite{Rostedt07}. They measured interrupt
and scheduling latencies of a periodic task. However, their experiments were
conducted without processor load and the methodology used was not precisely
described. Siro et al \cite{Siro07} compares Linux$^{\mathbf{Prt}}$, RT-Linux
\cite{rtLinux} and Linux$^{\mathbf{RTAI}}$ \cite{RTAI} with LMbench \cite{McVoy96}
and by measuring the scheduling deviation of a periodic task. The authors tested the
systems with a load overhead, but they did not consider interrupt load. In their
website, the developers of the Adeos project \cite{Benoit05} present some
comparative results for \preemptt and Adeos. In their evaluation, they used LMbench
\cite{McVoy96} to characterize the performance of the two platforms and measured the
interrupt latencies gathered from the parallel port.

The interrupt latency results of our work are similar to those obtained by Benoit et
al \cite{Benoit05} for Linux$^{\mathbf{Xen}}$. However, our results differ from
their work for Linux$^{\mathbf{Prt}}$, without the option \cod{IRQF\_NODELAY} since
we noticed some degradation of time guarantees by the platform, as seen in Section
\ref{sec:latIrq}. Regarding activation latencies, we are not aware of any other
comparative work. Experiments similar to those reported here were conducted for
Linux$^{\mathbf{RTAI}}$ \cite{Regnier08b} and the results are similar to those
presented for Linux$^{\mathbf{Xen}}$, since both platforms use Adeos \nanokernel.

\section{Conclusion}
\label{sec:conc}

% Neste trabalho, a avaliação de duas soluções de SOTR baseadas em Linux foi
% realizada. A metodologia experimental permitiu medir as latências de interrupção e
% de ativação, em situações de carga variável, tanto do processador quanto de eventos
% externos tratados por interrupção.  Enquanto o Linux padrão apresentou latências no
% pior caso acima de $100 \mu s$, as plataformas Linux$^{\mathbf{Prt}}$ e
% Linux$^{\mathbf{Xen}}$ conseguiram prover garantias temporais com uma precisão
% abaixo de $20 \mu s$. No entanto, para se conseguir este comportamento em relação ao
% Linux$^{\mathbf{Prt}}$, foi necessário desabilitar \ing{threads} de interrupção,
% tornando o sistema menos flexível. Com tais \ing{threads}, o comportamento de
% Linux$^{\mathbf{Prt}}$ sofre considerável degradação da sua previsibilidade
% temporal.  A plataforma Linux$^{\mathbf{Xen}}$ se mostrou mais adequada, pois tanto
% oferece um ambiente de programação em modo usuário, quanto consegue previsibilidade
% temporal característica de sistema de tempo real.

%Luciano
In this work, we conducted a timing evaluation of two Linux-based RTOS. We performed
experiments to obtain precise interrupt and activation latencies in situations of
variable load, caused by both the processor and external events generated by
interrupts.  Our experimental scenarios considered local measurements (não entendi
daqui para frente) was compared with a more regular configuration based on external
measurements through the parallel port.  As both configuration gives similar
results, it appears that the first methodology is founded and can be efficiently
used for the purpose of real-time platforms comparisons.

While the standard Linux kernel presented worst case latencies over $100 \mu s$,
Linux$^{\mathbf{Prt}}$ and Linux$^{\mathbf{Xen}}$ managed to provide temporal
guarantees within $20 \mu s$. However, in order to achieve this behaviour in
Linux$^{\mathbf{Prt}}$, we had to disable interrupt threading for the parallel port
handler, which turns the system less flexible (POR QUE MENOS FLEXIVEL ?). Overall,
Linux$^{\mathbf{Xen}}$ showed better results in achieving the temporal
predictability needed by current real-time systems.

%Paul
% In this work, the evaluation of two RTOS solutions based on Linux was held. The
% methodology has allowed experimental measurements of interrupt and activation
% latencies, in situations of variable load, both of the processor and of external
% events processed by interruption.  Two experimental set up were used, the first
% based on local measurements was compared with a more regular set up based on
% external measurements through the parallel port.  As both experiments gives
% similar results, it appears that the first methodology is founded and can be
% efficiently used for the purpose of real-time platforms comparisons.

% While the standard Linux presented latencies in the worst case over $100 \mu s$, the
% platforms Linux$^{\mathbf{Prt}}$ and Linux$^{\mathbf{Xen}}$ managed to provide
% temporal guarantees with a precision below $20 \mu s$. However, in ordrer to achieve
% this behaviour with Linux$^{\mathbf{Prt}}$, it was necessary to disable the
% interruption threading for the parallel port IRQ line, making the system less
% flexible. With such thread, the behavior of Linux$^{\mathbf{Prt}}$ suffers
% considerable deterioration of its temporal predictability. The Linux platform
% Linux$^{\mathbf{Xen}}$ was found more appropriate since offers a user-mode
% programming environment as well as temporal predictability characteristic of
% real-time system.

%\end{multicols}
% \small
\bibliographystyle{abbrv}
\bibliography{bib}

\end{document}



\begin{figure*}%
 \centering
 \subfloat[\small{\textbf{Linux$^{\mathbf{Std}}$ - without load} \newline
 \vskip 1mm M:    18.0, SD:     0.3,  Mn:     9.6, Mx:    27.3 }]{%
   \label{fig:ker23Sem}%
   {\scalebox{0.67}{\input{fig2/ker23Sem}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Std}}$ - with loads} \newline
 \vskip 1mm M:    21.8, SD:     3.2,  Mn:     9.8, Mx:    80.6 }]{%
   \label{fig:ker23Tot}%
   {\scalebox{0.67}{\input{fig2/ker23Tot}}}}%

 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - without load} \newline
 \vskip 1mm M:    19.4, SD:     0.5,  Mn:     9.8, Mx:    42.2 }]{%
   \label{fig:preSem}%
   {\scalebox{0.67}{\input{fig2/preSem}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - with loads} \newline
 \vskip 1mm M:    41.9, SD:    18.9,  Mn:     9.7, Mx:   324.4 }]{%
   \label{fig:preTot}%
   {\scalebox{0.67}{\input{fig2/preTot}}}}%

 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - without load, option
     \cod{IRQF\_NODELAY}} \newline
   \vskip 1mm M:    18.1, SD:     0.3,  Mn:     9.1, Mx:    25.5 }]{%
   \label{fig:preSemSchedND}%
   {\scalebox{0.67}{\input{fig2/preSemND}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - with loads, option
     \cod{IRQF\_NODELAY}} \newline
   \vskip 1mm M:    21.2, SD:     2.4,  Mn:     9.1, Mx:    50.5  }]{%
   \label{fig:preTotSchedND}%
   {\scalebox{0.67}{\input{fig2/preTotND}}}}

 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - without load} \newline
 \vskip 1mm M:    18.2, SD:     0.3,  Mn:     9.1, Mx:    23.1 }]{%
   \label{fig:xenSem}%
   {\scalebox{0.67}{\input{fig2/xenSem}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - with loads} \newline
 \vskip 1mm M:    20.7, SD:     1.5,  Mn:     9.1, Mx:    30.0 }]{%
   \label{fig:xenTot}%
   {\scalebox{0.67}{\input{fig2/xenTot}}}}%

 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - without load} \newline
 \vskip 1mm M:     6.5, SD:     0.5,  Mn:     5.0, Mx:    13.2 }]{%
   \label{fig:xenSemSched}%
   {\scalebox{0.67}{\input{fig2/xenSemSched}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - with loads} \newline
 \vskip 1mm M:    11.1, SD:     3.0,  Mn:     5.0, Mx:    25.7 }]{%
   \label{fig:xenTotSched}%
   {\scalebox{0.67}{\input{fig2/xenTotSched}}}}

 \caption[Interrupt latencies]{Interrupt latencies with PP-IRQ triggered on $E_M$
   by $E_T$ at a $20 Hz$ frequency.}
 \label{fig:latIrq2}%
\end{figure*}

\begin{figure*}%
 \centering

 \subfloat[\small{\textbf{Linux$^{\mathbf{Std}}$ - without load} \newline
 \vskip 1mm M:     5.7, SD:     0.6,  Mn:     5.0, Mx:    15.6 }]{%
   \label{fig:ker23SemSched}%
   {\scalebox{0.67}{\input{fig2/ker23SemSched}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Std}}$ - with loads}\newline
 \vskip 1mm M:    23.3, SD:    28.5,  Mn:     5.0, Mx:   623.3 }]{%
   \label{fig:ker23TotSched}%
   {\scalebox{0.67}{\input{fig2/ker23TotSched}}}}

 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - without load} \newline
 \vskip 1mm M:     6.1, SD:     0.6,  Mn:     5.0, Mx:    17.3 }]{%
   \label{fig:preSemSched}%
   {\scalebox{0.67}{\input{fig2/preSemSched}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - with loads} \newline
 \vskip 1mm M:    10.1, SD:     3.7,  Mn:     5.0, Mx:    70.5 }]{%
   \label{fig:preTotSched}%
   {\scalebox{0.67}{\input{fig2/preTotSched}}}}

 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - without load, option
     \cod{IRQF\_NODELAY}} \newline
 \vskip 1mm M:     6.8, SD:     0.4,  Mn:     5.1, Mx:    15.4 }]{%
   \label{fig:preSemSchedND}%
   {\scalebox{0.67}{\input{fig2/preSemSchedND}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Prt}}$ - with loads, option
     \cod{IRQF\_NODELAY}} \newline
 \vskip 1mm M:    15.6, SD:     6.6,  Mn:     5.0, Mx:   107.2}]{%
   \label{fig:preTotSchedND}%
   {\scalebox{0.67}{\input{fig2/preTotSchedND}}}}

 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - without load} \newline
 \vskip 1mm M:     6.5, SD:     0.5,  Mn:     5.0, Mx:    13.2 }]{%
   \label{fig:xenSemSched}%
   {\scalebox{0.67}{\input{fig2/xenSemSched}}}} \hspace{10pt}%
 \subfloat[\small{\textbf{Linux$^{\mathbf{Xen}}$ - with loads} \newline
 \vskip 1mm M:    11.1, SD:     3.0,  Mn:     5.0, Mx:    25.7 }]{%
   \label{fig:xenTotSched}%
   {\scalebox{0.67}{\input{fig2/xenTotSched}}}}

 \caption[Activation latencies]{Activation latencies with PP-IRQ triggered on $E_M$
   by $E_T$ at a $20 Hz$ frequency.}
 \label{fig:latAtiv2}%
\end{figure*}


