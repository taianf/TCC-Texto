\xchapter{Base teórica}{}

Neste capítulo será abordado a teoria dos sistema de tempo real, como o Linux trata interrupções e os tipos de mecanismos para tal. Também veremos alguns trabalhos relacionados.

\section{Sistemas Operacionais de tempo real}

Entre os desenvolvedores há um ditado que diz \say{um sistema de tempo real é um sistema que faz o que você espera que ele faça no tempo que você espera que ele faça}. Assim, qualquer sistema pode ser considerado um sistema de tempo real dado uma restrição temporal suficiente para realizar a sua tarefa.

Em alguns casos, a realização da tarefa fora do tempo esperado é apenas desagradável, mas em outros casos esse atraso pode comprometer toda a operação do sistema. Em um sistema de controle ou segurança, por exemplo, um atraso em uma ação específica, como acionar um alarme ou ativar um controle de incêndio, pode levar a mortes. Assim, Laplante define um sistema de tempo real como \say{um sistema de tempo real é aquele que deve satisfazer explicitamente restrições de tempo de resposta podendo ter consequências de risco ou falha não satisfazendo às suas restrições} \cite{Laplante2004}. 

O tempo entre o início de um evento e o momento em que os seus efeitos se tornam perceptíveis é chamado de latência. Em sistemas operacionais de propósito geral, a latência pode não ter limites em situações específicas, por exemplo, pode ser que uma tarefa de escrita só possa ocorrer apenas quando não houver tarefas de leitura na fila. Como tarefas de leituras podem continuar chegando arbitrariamente, a tarefa de escrita pode nunca ser executada.

Em um sistema em tempo real, existem técnicas para impedir que essas situações ocorram. É possível atribuir \textit{deadlines} a cada tarefa recebida e priorizar aquelas com um \textit{deadline} mais próximo. Quando as tarefas são periódicas e conhecidas, você pode atribuir prioridades fixas.

Um dos desafios dos sistemas operacionais de tempo real é que a maioria das aplicações possui tarefas com restrições em temporal e tarefas que não possuem restrições de tempo. Ambas as classes de tarefas precisam ser executadas ao mesmo tempo e talvez se comunicar entre si com níveis mistos de criticidade \cite{Cartwrigh2018}.

\section{Linux}

O Linux é um sistema operacional de propósito geral de código aberto criado por Linus Torvalds em 1991. Seu modelo de licença e desenvolvimento fez do Linux o projeto de código aberto com uma das maiores comunidades de desenvolvedores. Em 2000, a Linux Foundation, uma organização sem fins lucrativos para promover o crescimento do Linux, foi fundada. Várias empresas fazem parte da Linux Foundation e também estão colaborando com o projeto, incluindo, entre outras, empresas como Google, Cisco, Intel, IBM, Oracle e, mais recentemente, a Microsoft \cite{LinuxFoundation}. 

\section{Interrupções}

Os computadores modernos precisam executar várias atividades ao mesmo tempo e também verificar estímulos externos. Ao digitar no teclado, espera-se que o caractere seja exibido na tela imediatamente. Para isso, a CPU teria que verificar frequentemente se há alguma tecla pressionada. Uma varredura com pouca frequência tornaria o tempo de espera muito longo, enquanto uma varredura muito frequente levaria a CPU a desperdiçar seu ciclo de trabalho, verificando que nada externo ocorreu. \cite{Rothberg2015}

Para evitar esse desperdício de recursos, a CPU delega algumas atividades para outros hardware, como controladores USB, que podem enviar sinais assíncronos à CPU, avisando que há novas tarefas. Esses sinais são chamados de interrupções. Quando a CPU recebe uma interrupção, interrompe o trabalho e transfere o controle para uma função instalada anteriormente para lidar com esse evento. Quando esse evento é tratado, a CPU retoma o trabalho que foi interrompido. Assim, qualquer programa comum pode ser interrompido em praticamente qualquer ponto, pois o tratamento de interrupções tem prioridade. \cite{LinuxDeviceDrivers}

Enquanto o tratamento de uma interrupção estiver ocorrendo, novas interrupções podem ocorrer. Isso tem o potencial de fazer com que uma interrupção nunca seja tratada ou que nunca execute o programa novamente. Para isso, o kernel é capaz de mascarar interrupções. Ou seja, ele pode adiar o tratamento por uma interrupção enquanto outro tratamento já está sendo realizado e, após terminar o primeiro tratamento, tratar as novas interrupções.

Os sistemas operacionais têm três casos de uso para tratadores de interrupção. Primeiro, quando uma aplicação faz uma chamada de sistema, ele precisa alternar do espaço do usuário para o espaço do kernel de maneira segura. Segundo, quando ocorre um erro em tempo de execução, como acesso ilegal à memória, que precisa de uma rotina para lidar com esse erro e decidir como proceder ou se o programa deve ser encerrado. Esses dois casos são interrupções síncronas ou de software, pois é possível prever quando elas ocorrerão. E, finalmente, quando o hardware externo envia um comando assíncrono para a CPU, conforme explicado no parágrafo anterior, que é uma interrupção assíncrona ou uma interrupção do hardware. \cite{LinuxInterrupts}

\section{O modelo de prólogo e epílogo}

Enquanto um tratador de interrupções está em execução, as interrupções são mascaradas e o tratamento é adiado após a conclusão desse tratador. Isso simplifica o código do tratador, pois não precisa lidar com casos em que foi interrompido. Além disso, permitir que um tratador seja interrompido pode causar um loop de recursão infinito, que em máquinas com memória finita pode causar um estouro de pilha. Portanto, é preferível adiar o tratamento de novas interrupções, mesmo que isso cause possíveis perdas de interrupção quando várias chegarem ao mesmo tempo.

Para impedir que essa perda de interrupção ocorra, os tratadores devem ser pequenos. Isso vai contra a essência de algumas interrupções, como ler e gravar no disco, que são tarefas demoradas. No entanto, essas tarefas não precisam ser executadas imediatamente, nem precisam desativar outras interrupções. Assim, o modelo de prólogo e epílogo oferece uma maneira de contornar essa limitação, quebrando o tratador em duas partes, tratando apenas a parte crítica com as interrupções desativadas e adiando o máximo de trabalho possível para um contexto em que as interrupções são ativadas novamente. No prólogo, a parte crítica do tratador é executada e a as demais partes são tratadas no epílogo, onde as interrupções estão novamente habilitadas. No Linux, eles também são também chamados de \textit{top half} e \textit{bottom half}. \cite{OReilly}

Durante a execução do prólogo, as interrupções são mascaradas. Quando o prólogo lida com a parte crítica, ele enfileira a parte não crítica da interrupção a ser executada no epílogo. Durante o epílogo, as interrupções são ativadas novamente e podem sofrer outras interrupções. Se ocorrerem, o prólogo da nova interrupção será tratado imediatamente e o epílogo será enfileirado e tratado após o tratamento do epílogo da primeira interrupção. O tempo decorrido entre o evento até o prólogo começar a tratar a interrupção é chamado de latência de interrupção, enquanto o tempo decorrido entre o evento até o epílogo começar a tratar a interrupção é chamado de latência de ativação.

Essa divisão é comum em muitos sistemas operacionais. O Windows tem sua implementação, que é Chamada de \textit{Deferred Procedure Call}, ou Chamada de Procedimentos Deferidos, para enfileirar tarefas de baixa prioridade \cite{InsideMicrosoftWindows}. No Linux, existem 3 tipos de tratamento para o epílogo, que serão detalhados na próxima sessão.

\section{Tipos de epílogo no Linux}

Depois de tratar o prólogo, o Linux tem três maneiras de lidar com o trabalho não crítico que foi atrasado. Os Softirqs são a maneira mais eficiente, mas menos flexível, pois é implementada diretamente no código do kernel. Os Tasklets são uma extensão do Softirq e são criados sobre ele. É menos eficiente, mas pode ser implementado como um módulo do kernel. E, finalmente, temos os Workqueue, que são tratadas no contexto dos processos e não no kernel. Isso os torna muito flexíveis, mas menos performáticos. \cite{OReilly}

\subsection{Softirq}

Softirqs são mecanismos simples para transferir trabalho de um contexto ininterruptível para um contexto interruptível. Eles estão associados a tarefas de rede, temporizadores e outras tarefas críticas e são a base dos Tasklets discutidos na próxima sessão. Os softirqs são executados assim que o sistema operacional muda do espaço do usuário para o espaço do kernel ou retorna de uma interrupção. Softirqs são especificados em tempo de compilação e não podem ser alocados em tempo de execução. Há um limite para quantos Softirqs estão implantados, atualmente em 32, já estão em um vetor específico para isso. A prioridade de um Softirq é a posição dentro do vetor.

Quando um Softirq é iniciado, ele é executado até concluir a tarefa. Isso significa que ele só pode ser interrompido por outra interrupção. O código que gerencia a execução do Softirq pode ser executado em vários núcleos da CPU; portanto, o desenvolvedor é responsável por garantir a sincronização entre os dados. Como não há garantia de que este Softirq tenha um contexto de processo associado, esse tratador não pode dormir e, como o tratador é executado até sua conclusão, é ideal que ele não consuma muito tempo da CPU.

Embora outras interrupções sejam tratadas, nenhuma outra tarefa será tratada até que o Softirq seja concluído. Portanto, mesmo se o tratamento de interrupções for rápido, se houver muitos Softirqs pendentes, as tarefas poderão ser adiadas indefinidamente. Para evitar esse comportamento, sempre que um tratador é finalizado, o tempo de duração da tratador da fila Softirqs é verificado. Se este tempo exceder um limite, a iteração na fila será interrompida. Em vez disso, uma thread de kernel dedicada, \textit{ksoftirqd}, é criada para lidar com essas interrupções. \cite{OReilly, Rothberg2015}

\subsection{Tasklet}

Enquanto o Softirq é simplesmente um índice de tarefas a serem executadas, os Tasklets são ponteiros para estruturas que contêm informações semelhantes, mas isso permite que as estruturas sejam alocadas em qualquer lugar do kernel. Isso permite que esses mecanismos de interrupção sejam implementados como módulos do kernel. Também não há restrições quanto ao número de Tasklets que podem ser implantados.

A estrutura do Tasklet consiste basicamente no tratador associado a ela e no argumento passado a ela. Ao chamar um Tasklet, essa estrutura é enfileirada em uma lista encadeada e marcada com um Softirq específico. Existem duas filas de Tasklet, uma de baixa prioridade e outra de alta prioridade. Ao executar esse Softirq, ele itera sobre as listas do Tasklet e chama o tratador específico para cada um. Portanto, os Tasklets estão no mesmo contexto que o Softirqs, sem processo associado e incapazes de serem postos para dormir. Como o Softirqs, eles são executados na CPU na qual foram chamados para aumentar a localidade dos dados e melhorar a utilização do cache.

Antes de chamar o tratador de um Tasklet, você deve bloquear sua estrutura. Quando essa estrutura já está bloqueada, esse tratador é colocado na fila novamente e o Softirq que verifica os Tasklets é novamente marcado como pendente. Isso implica que apenas um Tasklet pode ser executado na CPU a qualquer momento e simplifica o código necessário além de evitar conflitos de concorrência.

Os tasklets são mais fáceis de usar e mais flexíveis e podem ser usados em qualquer lugar do kernel. Por serem baseados no Softirq, eles têm as mesmas limitações de não poderem ser postos para dormir. Essa limitação não existe nos Workqueue, que serão tratados abaixo. \cite{OReilly, Rothberg2015}

\subsection{Workqueue}

Os Workqueues são usados quando precisamos tratar uma tarefa fora do contexto de interrupção e essa tarefa dorme ou fica bloqueada. A tarefa a ser executada é encapsulada e enfileirada na fila de Workqueues. Threads dedicadas executam as tarefas nesta fila. Quando essa tarefa precisa dormir ou ser bloqueada, o escalonador de tarefas do sistema operacional pode simplesmente alterar a tarefa a ser executada. Isso aumenta a capacidade de resposta, impedindo que os threads do usuário parem enquanto são tratadas interrupções longas.

Como o comprimento da fila é variável, o número de threads dedicadas à execução dos itens da fila de Workqueues precisa ser dinâmico para evitar desperdício de recursos. Essa fila é primeiro organizada em filas secundárias, pelo menos uma fila de alta prioridade e uma fila de baixa prioridade, cada uma com uma thread associada. Quando uma fila secundária fica vazia, a thread é colocada em suspensão. Quando novos itens são adicionados, a thread é acordada para processar esses novos itens. Para evitar desperdiçar memória, quando uma thread gasta muito tempo dormindo, é destruída. No entanto, sempre há pelo menos uma thread em espera para processar interrupções na fila. Por outro lado, quando a fila está crescendo e o uso da CPU não é maximizado, novas threads são criadas.

A capacidade de dormir ou ser bloqueada torna os Workqueues muito mais flexíveis do que outros mecanismos. O custo é que essas tarefas são tratadas por threads gerenciados pelo escalonador, tendo um custo para gerenciar. Os Workqueues também podem ser executados em qualquer núcleo, dificultando a obtenção das informações necessárias em cache. \cite{OReilly, Rothberg2015}

\section{Trabalhos relacionados}

Em 2003, Matthew Wilcox \cite{Wilcox2003} apresentou estruturas de interrupção e como elas fazem para enfileirar tarefas de interrupção que precisam ser tratadas fora da zona de tempo crítico e como essas estruturas gerenciam os dados.

Em 2008, no Workshop de Sistemas Operacionais, Paul Regnier et al \cite{Regnier2008} apresentaram uma avaliação das interrupções no Linux. Eles compararam a latência de ativação no kernel padrão, Preempt-RT e Xenomai. Esses 2 patches em tempo real para Linux serão abordados no próximo capítulo. Eles avaliaram os tempos de latência da interrupção, o tempo até a execução do prólogo e a latência de ativação, o tempo até a execução do epílogo, usando 3 estações em rede para realizar as medições, sendo uma estação de medição, uma estação para gerar eventos de interrupção na estação de medição e outra para gerar uma carga na estação de medição. Eles chegaram à conclusão de que o Xenomai era muito mais estável quando a estação de medição estava sob uma carga de trabalho.

Em 2016, Tomaž Šolc comparou o tempo de resposta do Raspberry com o tempo de resposta do Arduino \cite{Solc2016}. Ele realizou testes usando um Arduino Uno e um Raspberry Pi Zero. O Arduino foi testado tanto para interrupções de hardware quanto para um laço de verificação. Os testes no Raspberry foram feitos com um módulo do kernel, um programa C nativo e outro programa Python. O Arduino, sendo um microcontrolador, alcançou tempos de resposta quase constantes quando usado com interrupções e uma pequena variação ao verificar dentro do loop. O Raspberry teve tempos de resposta comparável quando no modo kernel, mas com valores oscilantes, tornando-o menos previsível.

Em 2018, Gustav Johansson \cite{Johansson2018} fez uma avaliação da latência do Raspberry usando o Xenomai. Ele fez várias análises, mas testou apenas o Xenomai, sem comparar com outros sistemas. Ele usou um kernel pré-compilado para realizar as medições, pois teve vários problemas para aplicar o patch e compilar o kernel. Inicialmente, vários problemas foram encontrados na compilação do kernel com o Xenomai durante o curso deste trabalho, e o guia que Gustav forneceu no apêndice do seu trabalho foi essencial para ter sucesso na minha compilação.

Também em 2018, Luis Gerhorst \cite{Gerhorst2018} construiu uma ferramenta para executar análise de latência de interrupção no Linux, que é dividida nos módulos INTSpect e INTSight. Esta ferramenta desenvolvida por ele foi usada para realizar as medições deste trabalho. Em seu trabalho, ele mostrou a viabilidade da ferramenta testando em uma placa \textit{SAMA5D3 Xplained}. Esta placa possui um processador ARM de núcleo único, enquanto o Raspberry possui quatro núcleos em sua CPU.

